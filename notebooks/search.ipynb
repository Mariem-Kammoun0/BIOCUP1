{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49821366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection status: green points: 5104\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\..\\\\data\\\\input\\\\embeddings\\\\meta.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_parquet(path)\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_csv(path)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m meta = \u001b[43mread_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMETA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m dense = np.load(DENSE_PATH)\n\u001b[32m     75\u001b[39m sp = np.load(SPARSE_PATH, allow_pickle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mread_meta\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_meta\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> pd.DataFrame:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m path.endswith(\u001b[33m\"\u001b[39m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_csv(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Melek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:671\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, dtype_backend, filesystem, filters, to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    668\u001b[39m impl = get_engine(engine)\n\u001b[32m    669\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Melek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:253\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, dtype_backend, storage_options, filesystem, to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    241\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    242\u001b[39m     path,\n\u001b[32m   (...)\u001b[39m\u001b[32m    249\u001b[39m     **kwargs,\n\u001b[32m    250\u001b[39m ) -> DataFrame:\n\u001b[32m    251\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_pandas_metadata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    260\u001b[39m         pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    261\u001b[39m             path_or_handle,\n\u001b[32m    262\u001b[39m             columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    265\u001b[39m             **kwargs,\n\u001b[32m    266\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Melek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Melek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:935\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    926\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    927\u001b[39m             handle,\n\u001b[32m    928\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    931\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    932\u001b[39m         )\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m     handles.append(handle)\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '..\\\\..\\\\data\\\\input\\\\embeddings\\\\meta.parquet'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BioCUP â€” End-to-end Search Test (multi-chunks -> % primary_site)\n",
    "# Compatible with qdrant-client where query_points uses: query=..., using=\"dense\"/\"sparse\"\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qm\n",
    "\n",
    "# ----------------------------\n",
    "# 0) CONFIG\n",
    "# ----------------------------\n",
    "load_dotenv()\n",
    "\n",
    "COLLECTION = \"biocup_hybrid_splade_v1\"\n",
    "\n",
    "# Input embeddings (3 files)\n",
    "INPUT_EMB_DIR = os.path.normpath(\"../data/input/embeddings\")  # adapte si besoin\n",
    "META_PATH  = os.path.join(INPUT_EMB_DIR, \"meta.parquet\")        # ou meta.csv\n",
    "DENSE_PATH = os.path.join(INPUT_EMB_DIR, \"dense.npy\")\n",
    "SPARSE_PATH = os.path.join(INPUT_EMB_DIR, \"sparse_splade.npz\")\n",
    "\n",
    "# Search params\n",
    "K_DENSE = 40\n",
    "K_SPARSE = 40\n",
    "K_FUSED = 30\n",
    "RRF_K = 60\n",
    "\n",
    "# Evidence per predicted site\n",
    "MAX_EVIDENCE_PER_SITE = 5\n",
    "\n",
    "# Section weights (tunable)\n",
    "SECTION_WEIGHT = {\n",
    "    \"IHC\": 1.5,\n",
    "    \"DIAGNOSIS\": 1.3,\n",
    "    \"SYNOPTIC\": 1.2,\n",
    "    \"LYMPH_NODES\": 1.0,\n",
    "    \"LYMPH\": 1.0,\n",
    "    \"MARGINS\": 1.0,\n",
    "    \"MICRO\": 0.9,\n",
    "    \"GROSS\": 0.7,\n",
    "    \"COMMENT\": 0.6,\n",
    "    \"GENERAL\": 0.6,\n",
    "}\n",
    "\n",
    "def w_section(section: str) -> float:\n",
    "    return SECTION_WEIGHT.get((section or \"GENERAL\").upper(), 1.0)\n",
    "\n",
    "# ----------------------------\n",
    "# 1) CONNECT QDRANT\n",
    "# ----------------------------\n",
    "client = QdrantClient(\n",
    "    url=os.environ[\"QDRANT_URL\"],\n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "# quick health check\n",
    "info = client.get_collection(COLLECTION)\n",
    "print(\"Collection status:\", info.status, \"points:\", info.points_count)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) LOAD INPUT EMBEDDINGS\n",
    "# ----------------------------\n",
    "def read_meta(path: str) -> pd.DataFrame:\n",
    "    if path.endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "meta = read_meta(META_PATH)\n",
    "dense = np.load(DENSE_PATH)\n",
    "sp = np.load(SPARSE_PATH, allow_pickle=True)\n",
    "sp_indices = sp[\"indices\"]\n",
    "sp_values = sp[\"values\"]\n",
    "\n",
    "N = len(meta)\n",
    "assert dense.shape[0] == N == len(sp_indices) == len(sp_values), \"âŒ Input files misaligned\"\n",
    "print(\"âœ… Loaded input chunks:\", N, \"dense shape:\", dense.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) FILTERS (light guidance only)\n",
    "# ----------------------------\n",
    "def build_light_filter(chunk_section: str) -> qm.Filter:\n",
    "    must = []\n",
    "    must_not = [qm.FieldCondition(key=\"is_admin_noise\", match=qm.MatchValue(value=1))]\n",
    "\n",
    "    sec = (chunk_section or \"\").upper()\n",
    "    if sec == \"IHC\":\n",
    "        must.append(qm.FieldCondition(key=\"has_ihc\", match=qm.MatchValue(value=True)))\n",
    "    elif sec in (\"LYMPH_NODES\", \"LYMPH\"):\n",
    "        must.append(qm.FieldCondition(key=\"has_lymph\", match=qm.MatchValue(value=True)))\n",
    "    elif sec == \"MARGINS\":\n",
    "        must.append(qm.FieldCondition(key=\"has_margins\", match=qm.MatchValue(value=True)))\n",
    "\n",
    "    return qm.Filter(must=must, must_not=must_not)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) RRF FUSION (hybrid in code)\n",
    "# ----------------------------\n",
    "def rrf_fuse(dense_points, sparse_points, k=RRF_K):\n",
    "    \"\"\"\n",
    "    Rank-based fusion: sum 1/(k+rank+1)\n",
    "    Returns: fused_list (PointStruct-like), id2score, id2payload\n",
    "    \"\"\"\n",
    "    id2score = {}\n",
    "    id2payload = {}\n",
    "\n",
    "    for rank, p in enumerate(dense_points):\n",
    "        pid = p.id\n",
    "        id2score[pid] = id2score.get(pid, 0.0) + 1.0/(k + rank + 1)\n",
    "        if pid not in id2payload and p.payload is not None:\n",
    "            id2payload[pid] = p.payload\n",
    "\n",
    "    for rank, p in enumerate(sparse_points):\n",
    "        pid = p.id\n",
    "        id2score[pid] = id2score.get(pid, 0.0) + 1.0/(k + rank + 1)\n",
    "        if pid not in id2payload and p.payload is not None:\n",
    "            id2payload[pid] = p.payload\n",
    "\n",
    "    fused_ids = [pid for pid, _ in sorted(id2score.items(), key=lambda x: x[1], reverse=True)]\n",
    "    return fused_ids, id2score, id2payload\n",
    "\n",
    "# ----------------------------\n",
    "# 5) END-TO-END SEARCH + AGGREGATION\n",
    "# ----------------------------\n",
    "scores_by_site = defaultdict(float)\n",
    "evidence_by_site = defaultdict(list)\n",
    "\n",
    "# prevent the same case from over-voting inside the SAME input chunk\n",
    "best_case_contrib_per_chunk = {}  # (chunk_i, case_id) -> best score\n",
    "\n",
    "for i in range(N):\n",
    "    row = meta.iloc[i]\n",
    "    sec = str(row.get(\"section\", \"GENERAL\"))\n",
    "    sec_w = w_section(sec)\n",
    "\n",
    "    q_dense = dense[i].tolist()\n",
    "    q_sparse = qm.SparseVector(\n",
    "        indices=sp_indices[i].tolist(),\n",
    "        values=sp_values[i].tolist()\n",
    "    )\n",
    "\n",
    "    flt = build_light_filter(sec)\n",
    "\n",
    "    # ---- DENSE QUERY (your version: query=..., using=\"dense\")\n",
    "    dense_res = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=q_dense,\n",
    "        using=\"dense\",\n",
    "        query_filter=flt,\n",
    "        limit=K_DENSE,\n",
    "        with_payload=True\n",
    "    )\n",
    "    dense_points = dense_res.points or []\n",
    "\n",
    "    # ---- SPARSE QUERY (your version: query=SparseVector, using=\"sparse\")\n",
    "    sparse_res = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=q_sparse,\n",
    "        using=\"sparse\",\n",
    "        query_filter=flt,\n",
    "        limit=K_SPARSE,\n",
    "        with_payload=True\n",
    "    )\n",
    "    sparse_points = sparse_res.points or []\n",
    "\n",
    "    # ---- HYBRID (RRF fusion)\n",
    "    fused_ids, id2rrf, id2payload = rrf_fuse(dense_points, sparse_points, k=RRF_K)\n",
    "    fused_ids = fused_ids[:K_FUSED]\n",
    "\n",
    "    # ---- Aggregate votes by primary_site\n",
    "    for pid in fused_ids:\n",
    "        payload = id2payload.get(pid)\n",
    "        if not payload:\n",
    "            continue\n",
    "\n",
    "        site = payload.get(\"primary_site\")\n",
    "        case_id = payload.get(\"case_id\")\n",
    "        if not site or not case_id:\n",
    "            continue\n",
    "\n",
    "        contrib = id2rrf.get(pid, 0.0) * sec_w\n",
    "\n",
    "        key = (i, str(case_id))\n",
    "        if contrib <= best_case_contrib_per_chunk.get(key, 0.0):\n",
    "            continue\n",
    "        best_case_contrib_per_chunk[key] = contrib\n",
    "\n",
    "        scores_by_site[site] += contrib\n",
    "\n",
    "        # keep evidence snippets\n",
    "        if len(evidence_by_site[site]) < MAX_EVIDENCE_PER_SITE:\n",
    "            txt = payload.get(\"chunk_text\", \"\")\n",
    "            snippet = (txt[:180] + \"...\") if isinstance(txt, str) and len(txt) > 180 else txt\n",
    "            evidence_by_site[site].append({\n",
    "                \"score\": contrib,\n",
    "                \"case_id\": str(case_id),\n",
    "                \"section\": payload.get(\"section\"),\n",
    "                \"snippet\": snippet\n",
    "            })\n",
    "\n",
    "# ----------------------------\n",
    "# 6) NORMALIZE -> % BY SITE\n",
    "# ----------------------------\n",
    "total = sum(v for v in scores_by_site.values() if v > 0)\n",
    "percent_by_site = {k: (v/total)*100.0 for k, v in scores_by_site.items()} if total > 0 else {}\n",
    "\n",
    "# sort results\n",
    "sorted_sites = sorted(percent_by_site.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"âœ… Predicted primary_site (%)\")\n",
    "print(\"==============================\")\n",
    "for site, pct in sorted_sites[:10]:\n",
    "    print(f\"{site:15s} {pct:6.2f}%\")\n",
    "\n",
    "# show evidence for top-1 and top-3\n",
    "print(\"\\n==============================\")\n",
    "print(\"ðŸ”Ž Evidence (top sites)\")\n",
    "print(\"==============================\")\n",
    "for site, pct in sorted_sites[:3]:\n",
    "    print(f\"\\n--- {site} ({pct:.2f}%) ---\")\n",
    "    ev = sorted(evidence_by_site[site], key=lambda d: d[\"score\"], reverse=True)\n",
    "    for e in ev:\n",
    "        print(f\"  score={e['score']:.4f} case={e['case_id']} sec={e['section']} | {e['snippet']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
