{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c57a5aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports OK âœ…\n"
     ]
    }
   ],
   "source": [
    "import qdrant_client\n",
    "import sentence_transformers\n",
    "import langchain\n",
    "import pandas\n",
    "\n",
    "print(\"All imports OK âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6b82b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install pandas python-dotenv qdrant-client sentence-transformers\n",
    "!pip -q install langchain langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9c2d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Qdrant: https://db948a35-ef44-4b88-ad47-a65eb335dcbb.us-east4-0.gcp.cloud.qdrant.io\n",
      "Collections: collections=[CollectionDescription(name='biocup_report_chunks_minilm384'), CollectionDescription(name='biocup_chunks')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "load_dotenv()  # loads .env from project root (same level as backend/, data/, notebooks/)\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  \n",
    "\n",
    "assert QDRANT_URL, \"Missing QDRANT_URL in .env\"\n",
    "assert QDRANT_API_KEY, \"Missing QDRANT_API_KEY in .env\"\n",
    "\n",
    "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "print(\"âœ… Connected to Qdrant:\", QDRANT_URL)\n",
    "print(\"Collections:\", client.get_collections())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8ef2f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['chunk_id', 'case_id', 'primary_site', 'tcga_type', 'patient_id',\n",
      "       'section', 'original_section', 'chunk_index', 'sub_index', 'chunk_text',\n",
      "       'has_tnm', 'has_size', 'has_ihc', 'has_lymph', 'has_margins',\n",
      "       'has_tumor_size_cue', 'is_admin_noise'],\n",
      "      dtype='str')\n",
      "Rows: 420\n",
      "Columns: ['case_id', 'primary_site', 'tcga_type', 'patient_id', 'patient_filename', 'report_text']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = ROOT / \"data\" / \"processed\" / \"biocup_chunks.csv\"\n",
    "chunk_df = pd.read_csv(CSV_PATH)\n",
    "print(chunk_df.columns)\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fba3b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEXT_COL = \"report_text\"\n",
    "assert TEXT_COL in df.columns, \"report_text column not found!\"\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16f9ad91",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'report_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BIOCUP1/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'report_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m embed_model = SentenceTransformer(\u001b[33m\"\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m vectors = embed_model.encode(\u001b[43mchunk_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTEXT_COL\u001b[49m\u001b[43m]\u001b[49m.tolist(), show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m).tolist()\n\u001b[32m      5\u001b[39m VECTOR_SIZE = \u001b[38;5;28mlen\u001b[39m(vectors[\u001b[32m0\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Embedded:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vectors))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BIOCUP1/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BIOCUP1/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'report_text'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "vectors = embed_model.encode(chunk_df[TEXT_COL].tolist(), show_progress_bar=True).tolist()\n",
    "VECTOR_SIZE = len(vectors[0])\n",
    "\n",
    "print(\"âœ… Embedded:\", len(vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b789522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Collection already exists: biocup_chunks\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models as qm\n",
    "\n",
    "VECTOR_SIZE = 384\n",
    "\n",
    "if not client.collection_exists(COLLECTION):\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config=qm.VectorParams(size=VECTOR_SIZE, distance=qm.Distance.COSINE),\n",
    "    )\n",
    "    print(\"âœ… Created collection:\", COLLECTION)\n",
    "else:\n",
    "    print(\"â„¹ï¸ Collection already exists:\", COLLECTION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad7cc472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… upserted 50/5089\n",
      "âœ… upserted 100/5089\n",
      "âœ… upserted 150/5089\n",
      "âœ… upserted 200/5089\n",
      "âœ… upserted 250/5089\n",
      "âœ… upserted 300/5089\n",
      "âœ… upserted 350/5089\n",
      "âœ… upserted 400/5089\n",
      "âœ… upserted 450/5089\n",
      "âœ… upserted 500/5089\n",
      "âœ… upserted 550/5089\n",
      "âœ… upserted 600/5089\n",
      "âœ… upserted 650/5089\n",
      "âœ… upserted 700/5089\n",
      "âœ… upserted 750/5089\n",
      "âœ… upserted 800/5089\n",
      "âœ… upserted 850/5089\n",
      "âœ… upserted 900/5089\n",
      "âœ… upserted 950/5089\n",
      "âœ… upserted 1000/5089\n",
      "âœ… upserted 1050/5089\n",
      "âœ… upserted 1100/5089\n",
      "âœ… upserted 1150/5089\n",
      "âœ… upserted 1200/5089\n",
      "âœ… upserted 1250/5089\n",
      "âœ… upserted 1300/5089\n",
      "âœ… upserted 1350/5089\n",
      "âœ… upserted 1400/5089\n",
      "âœ… upserted 1450/5089\n",
      "âœ… upserted 1500/5089\n",
      "âœ… upserted 1550/5089\n",
      "âœ… upserted 1600/5089\n",
      "âœ… upserted 1650/5089\n",
      "âœ… upserted 1700/5089\n",
      "âœ… upserted 1750/5089\n",
      "âœ… upserted 1800/5089\n",
      "âœ… upserted 1850/5089\n",
      "âœ… upserted 1900/5089\n",
      "âœ… upserted 1950/5089\n",
      "âœ… upserted 2000/5089\n",
      "âœ… upserted 2050/5089\n",
      "âœ… upserted 2100/5089\n",
      "âœ… upserted 2150/5089\n",
      "âœ… upserted 2200/5089\n",
      "âœ… upserted 2250/5089\n",
      "âœ… upserted 2300/5089\n",
      "âœ… upserted 2350/5089\n",
      "âœ… upserted 2400/5089\n",
      "âœ… upserted 2450/5089\n",
      "âœ… upserted 2500/5089\n",
      "âœ… upserted 2550/5089\n",
      "âœ… upserted 2600/5089\n",
      "âœ… upserted 2650/5089\n",
      "âœ… upserted 2700/5089\n",
      "âœ… upserted 2750/5089\n",
      "âœ… upserted 2800/5089\n",
      "âœ… upserted 2850/5089\n",
      "âœ… upserted 2900/5089\n",
      "âœ… upserted 2950/5089\n",
      "âœ… upserted 3000/5089\n",
      "âœ… upserted 3050/5089\n",
      "âœ… upserted 3100/5089\n",
      "âœ… upserted 3150/5089\n",
      "âœ… upserted 3200/5089\n",
      "âœ… upserted 3250/5089\n",
      "âœ… upserted 3300/5089\n",
      "âœ… upserted 3350/5089\n",
      "âœ… upserted 3400/5089\n",
      "âœ… upserted 3450/5089\n",
      "âœ… upserted 3500/5089\n",
      "âœ… upserted 3550/5089\n",
      "âœ… upserted 3600/5089\n",
      "âœ… upserted 3650/5089\n",
      "âœ… upserted 3700/5089\n",
      "âœ… upserted 3750/5089\n",
      "âœ… upserted 3800/5089\n",
      "âœ… upserted 3850/5089\n",
      "âœ… upserted 3900/5089\n",
      "âœ… upserted 3950/5089\n",
      "âœ… upserted 4000/5089\n",
      "âœ… upserted 4050/5089\n",
      "âœ… upserted 4100/5089\n",
      "âœ… upserted 4150/5089\n",
      "âœ… upserted 4200/5089\n",
      "âœ… upserted 4250/5089\n",
      "âœ… upserted 4300/5089\n",
      "âœ… upserted 4350/5089\n",
      "âœ… upserted 4400/5089\n",
      "âœ… upserted 4450/5089\n",
      "âœ… upserted 4500/5089\n",
      "âœ… upserted 4550/5089\n",
      "âœ… upserted 4600/5089\n",
      "âœ… upserted 4650/5089\n",
      "âœ… upserted 4700/5089\n",
      "âœ… upserted 4750/5089\n",
      "âœ… upserted 4800/5089\n",
      "âœ… upserted 4850/5089\n",
      "âœ… upserted 4900/5089\n",
      "âœ… upserted 4950/5089\n",
      "âœ… upserted 5000/5089\n",
      "âœ… upserted 5050/5089\n",
      "âœ… upserted 5089/5089\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'chunk_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BIOCUP1/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'chunk_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m TEXT_COL = \u001b[33m\"\u001b[39m\u001b[33mchunk_text\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m embed_model = SentenceTransformer(\u001b[33m\"\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m texts = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTEXT_COL\u001b[49m\u001b[43m]\u001b[49m.astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m     14\u001b[39m vectors = embed_model.encode(texts, show_progress_bar=\u001b[38;5;28;01mTrue\u001b[39;00m).tolist()\n\u001b[32m     16\u001b[39m points = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BIOCUP1/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/BIOCUP1/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'chunk_text'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "def upsert_in_batches(points, batch_size=50):\n",
    "    for start in range(0, len(points), batch_size):\n",
    "        batch = points[start:start + batch_size]\n",
    "        client.upsert(collection_name=COLLECTION, points=batch)\n",
    "        print(f\"âœ… upserted {start + len(batch)}/{len(points)}\")\n",
    "\n",
    "upsert_in_batches(points, batch_size=50)\n",
    "\n",
    "TEXT_COL = \"chunk_text\"\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "vectors = embed_model.encode(texts, show_progress_bar=True).tolist()\n",
    "\n",
    "points = []\n",
    "for i, row in df.reset_index(drop=True).iterrows():\n",
    "    payload = row.to_dict()\n",
    "    points.append(qm.PointStruct(id=i, vector=vectors[i], payload=payload))\n",
    "\n",
    "client.upsert(collection_name=COLLECTION, points=points)\n",
    "print(\"âœ… Uploaded:\", len(points), \"chunks to\", COLLECTION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02bb8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.692 | case_id: BIOCUP_00047 | section: LYMPH_NODES\n",
      "[case_id=BIOCUP_00047 | site=lung | type=LUAD | section=LYMPH_NODES] B. EMPHYSEMATOUS CHANGE WITH FOCAL ORGANIZING PNEUMONIA. C. MULTIPLE SMALL CARCINOID TUMORLETS. PART 2: LUNG, INFERIOR LEFT LOWER LOBE NODULE, WEDGE RE ...\n",
      "\n",
      "score: 0.663 | case_id: BIOCUP_00021 | section: LYMPH_NODES\n",
      "[case_id=BIOCUP_00021 | site=lung | type=LUSC | section=LYMPH_NODES] The lung adjacent to the tumor shows interstitial follicular pneumonia and early acute supporative pneumonia. The pleura is thick and hemorrahgic and a ...\n",
      "\n",
      "score: 0.658 | case_id: BIOCUP_00041 | section: MICRO\n",
      "[case_id=BIOCUP_00041 | site=lung | type=LUAD | section=MICRO] . Microscopic. Description. DIAGNOSIS Details. Comments. Laterality. LUNG TISSUE CHECKLIST. Left-upper. Specimen type: Lobectomy. Tumor site: Lung. Tumor siz ...\n",
      "\n",
      "score: 0.649 | case_id: BIOCUP_00007 | section: GROSS\n",
      "[case_id=BIOCUP_00007 | site=lung | type=LUSC | section=GROSS] There are n 0 satellite lesions, Peribronchial anthracotic lymph nodes are identified and submitted. The uninvolved lung paranchyma is unremarkable. The bron ...\n",
      "\n",
      "score: 0.646 | case_id: BIOCUP_00002 | section: SYNOPTIC\n",
      "[case_id=BIOCUP_00002 | site=lung | type=LUAD | section=SYNOPTIC] PRIMARY TUMOR (pT): pT1b: Tumor greater than 2 cm, but 3 cm or less in greatest dimension, surrounded by lung or visceral pleura, without bronchoscopic ev ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_points(resp):\n",
    "    # Case 1: resp is a tuple (points, something)\n",
    "    if isinstance(resp, tuple):\n",
    "        return resp[0]\n",
    "    # Case 2: resp is a QueryResponse-like object with .points\n",
    "    if hasattr(resp, \"points\"):\n",
    "        return resp.points\n",
    "    # Case 3: resp is already a list of points\n",
    "    return resp\n",
    "\n",
    "query = \"lung cancer case with no lymph node involvement\"\n",
    "qvec = embed_model.encode(query).tolist()\n",
    "\n",
    "resp = client.query_points(\n",
    "    collection_name=COLLECTION,\n",
    "    query=qvec,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "points = extract_points(resp)\n",
    "\n",
    "for pt in points:\n",
    "    p = pt.payload\n",
    "    print(\"score:\", round(pt.score, 3), \"| case_id:\", p.get(\"case_id\"), \"| section:\", p.get(\"section\"))\n",
    "    print(str(p.get(TEXT_COL))[:220], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71d56655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ cwd: /Users/mac/Documents/GitHub/BIOCUP1/notebooks\n",
      "ðŸ“Œ using .env at: /Users/mac/Documents/GitHub/BIOCUP1/.env | exists: True\n",
      "OPENAI_API_KEY loaded? True\n",
      "OPENAI_API_KEY preview: sk-proj...aaMA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1) Find repo root (folder that contains \"notebooks\")\n",
    "HERE = Path.cwd()\n",
    "ROOT = HERE if (HERE / \"notebooks\").exists() else HERE.parent\n",
    "\n",
    "# 2) Load .env explicitly from root (no guessing)\n",
    "dotenv_path = ROOT / \".env\"\n",
    "print(\"ðŸ“Œ cwd:\", HERE)\n",
    "print(\"ðŸ“Œ using .env at:\", dotenv_path, \"| exists:\", dotenv_path.exists())\n",
    "\n",
    "load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "\n",
    "# 3) Verify\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY loaded?\", bool(key))\n",
    "if key:\n",
    "    print(\"OPENAI_API_KEY preview:\", key[:7] + \"...\" + key[-4:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb935a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_points(resp):\n",
    "    # qdrant-client can return different shapes\n",
    "    if isinstance(resp, tuple):\n",
    "        return resp[0]\n",
    "    if hasattr(resp, \"points\"):\n",
    "        return resp.points\n",
    "    return resp\n",
    "\n",
    "def search(query: str, k: int = 5, query_filter=None):\n",
    "    qvec = embed_model.encode(query).tolist()\n",
    "    resp = client.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=qvec,\n",
    "        limit=k,\n",
    "        query_filter=query_filter,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return extract_points(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Case BIOCUP_00030**: Three lymph nodes from the right superior hilar were negative for tumor; matted lymph node from the right upper node was also negative; right posterior superior hilar lymph node was negative for tumor (N0) (case_id=BIOCUP_00030, chunk_index=0).\n",
      "  \n",
      "- **Case BIOCUP_00055**: Eighteen lymph nodes from level VII were negative for malignancy; one lymph node from level X was negative; one lymph node from level XI was negative; one lymph node from level XII was negative (N0) (case_id=BIOCUP_00055, chunk_index=0).\n",
      "\n",
      "- **Case BIOCUP_00039**: A lymph node biopsy from level 9 (left) was received, but no malignancy status was provided (case_id=BIOCUP_00039, chunk_index=16).\n",
      "\n",
      "- **Case BIOCUP_00007**: Out of two peribronchial lymph nodes, one was positive for metastasis; six hilar lymph nodes were negative (0/6); several other lymph nodes were reported as benign (case_id=BIOCUP_00007, chunk_index=1).\n",
      "\n",
      "- **Case BIOCUP_00016**: Four lymph nodes were examined, and none were positive for metastasis (0/4) (case_id=BIOCUP_00016, chunk_index=0).\n",
      "\n",
      "- **Case BIOCUP_00004**: One lymph node received with lobectomy specimen was negative for metastatic carcinoma (0/1); seven lymph nodes received with all specimen parts were also negative (0/7) (case_id=BIOCUP_00004, chunk_index=16).\n",
      "\n",
      "**Conclusion**: The lymph node status in these lung cancer reports predominantly indicates a negative status for malignancy (N0) across multiple cases, with only a few instances of positive findings.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a clinical assistant for BioCUP.\\n\"\n",
    "     \"Rules:\\n\"\n",
    "     \"- Use ONLY the provided context.\\n\"\n",
    "     \"- If context is insufficient, say what is missing.\\n\"\n",
    "     \"- Cite sources as (case_id, chunk_index).\\n\"\n",
    "     \"- Do not invent medical facts.\\n\"),\n",
    "    (\"human\",\n",
    "     \"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\n\"\n",
    "     \"Answer with bullet points + a short conclusion.\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def format_context(points, max_chars=3500):\n",
    "    out, total = [], 0\n",
    "    for pt in points:\n",
    "        p = pt.payload\n",
    "        block = (\n",
    "            f\"(case_id={p.get('case_id')}, chunk_index={p.get('chunk_index')}, score={pt.score:.3f})\\n\"\n",
    "            f\"{p.get('chunk_text','')}\"\n",
    "        )\n",
    "        out.append(block)\n",
    "        total += len(block)\n",
    "        if total > max_chars:\n",
    "            break\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "def rag(question: str, k: int = 6, query_filter=None):\n",
    "    points = search(question, k=k, query_filter=query_filter)\n",
    "    context = format_context(points)\n",
    "    answer = chain.invoke({\"question\": question, \"context\": context})\n",
    "    return answer, points\n",
    "\n",
    "answer, pts = rag(\"Summarize lymph node status mentioned in similar lung cancer reports.\", k=6)\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
